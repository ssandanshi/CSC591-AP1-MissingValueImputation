# -*- coding: utf-8 -*-
"""Final ML AP1 - Best Model Eval

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4mHDUf8OwNKaKjo1uCIOxqekcQYI8lq

# Mount Google Drive and Setup Environment
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import argparse
import warnings
from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler
from fancyimpute import MatrixFactorization, IterativeSVD, SimpleFill
warnings.simplefilter(action='ignore', category=Warning)
pd.reset_option('all')

"""# Parse Arguments
#### --train to train and evaluate nrms on train data
#### --test to test on test data
#### --tune to tune different values of k for KNN
"""
def get_args():
  parser = argparse.ArgumentParser()
  parser.add_argument('--train', action='store_true',
                      help='train and evaluate nrms on train data')
  parser.add_argument('--test', action='store_true',
                      help='test on test data')
  parser.add_argument('--tune', action='store_true',
                      help='tune different values of k for KNN')
  return parser.parse_args()


"""# Load Data
#### Continue from here if you already have the csv files. Otherwise run read data above.
"""
def load_data():
  df = pd.read_csv('joined_missing.csv',index_col=0)
  df2 = pd.read_csv('joined_groundtruth.csv',index_col=0)
  return df,df2


"""# Feature Engineering
### Add new relevant features to improve performance
### The following things are done:
### 1. Normalization - Min max scale values between 0 and 1 (per Patient)
### 2. Time Difference (Window) - Find time difference between k-windowed records per patient
### 3. Time Difference Median - Find time difference from the median time for each record per patient.

## Normalize
"""

def normalize(df_col,df_col_gt=None,returnmin=False):
  """
  Normalize given column using MinMaxScaling
  """
  min_val = np.min(df_col)
  max_val = np.max(df_col)
  df_col = df_col.map(lambda x: (x-min_val)/(max_val-min_val) if x else x)
  if df_col_gt is not None:
    df_col_gt = df_col_gt.map(lambda x: (x-min_val)/(max_val-min_val) if x else x)
    if returnmin:
      return df_col,df_col_gt,min_val,max_val
    else:
      return df_col,df_col_gt
  if returnmin:
    return df_col,min_val,max_val
  else:
    return df_col

"""## Time Difference (Window)"""

def time_diff_window(time,ptnum,window=None,median=False):
  timediff_all = []
  for i in set(ptnum):
    time_pat = time[ptnum==i]
    if median:
      timediff=[]
      median_time = time_pat.median()
      for j in range(len(time_pat)):
        timediff.append(abs(time_pat.iloc[j]-median_time))
      timediff_all.extend(timediff)
    else:
      timediff = [0]*window
      for j in range(window,len(time_pat)):
        timediff.append(time_pat.iloc[j]-time_pat.iloc[j-window])
      timediff_all.extend(timediff)
  return pd.Series(timediff_all)

"""### Perform all feature engineering on input dataframe"""

def feature_engg(inputdf,inputdf_gt=None,returnmin=False):
  inputdf["timediff_1"] = time_diff_window(inputdf["time"],inputdf["ptnum"],1)
  inputdf["timediff_5"] = time_diff_window(inputdf["time"],inputdf["ptnum"],5)
  inputdf["timediff_median"] = time_diff_window(inputdf["time"],inputdf["ptnum"],median=True)
  mins={}
  maxs={}
  for i in range(len(inputdf.columns)):
    if inputdf.columns[i] not in ['time','ptnum']:
      if inputdf_gt is not None:
        if not returnmin:
          inputdf.iloc[:,i], inputdf_gt.iloc[:,i] = normalize(inputdf.iloc[:,i],inputdf_gt.iloc[:,i],returnmin=returnmin)
        else:
          inputdf.iloc[:,i], inputdf_gt.iloc[:,i], min_val,max_val = normalize(inputdf.iloc[:,i],inputdf_gt.iloc[:,i],returnmin=returnmin)
      else:
        if not returnmin:
          inputdf.iloc[:,i] = normalize(inputdf.iloc[:,i],returnmin=returnmin)
        else:
          inputdf.iloc[:,i], min_val, max_val = normalize(inputdf.iloc[:,i],returnmin=returnmin)
      if returnmin:
        mins[inputdf.columns[i]] = min_val
        maxs[inputdf.columns[i]] = max_val
  
  if returnmin:
    return mins,maxs

"""### NRMS Evaluation criteria per Feature"""

def nrms(masked, imputed, groundtruth,ptnum):
  sum_values=0
  count=0
  for i in range(len(imputed)):
    if masked[i]:
      patient_vals = groundtruth[ptnum==ptnum[i]]
      max_v = max(patient_vals[~np.isnan(patient_vals)])
      min_v = min(patient_vals[~np.isnan(patient_vals)])
      count+=1
      sum_values += (abs(imputed[i]-groundtruth[i])/(max_v-min_v))**2
      
  return np.sqrt(sum_values/sum(masked))

"""# Impute Missing Values (Model Selection)
### Use fancyimpute library and various models inside the library to find the best model per analyte. The following models are tested:
### 1. SimpleFill (substitute with mean)
### 2. KNN
### 3. SoftImpute
### 4. BiScaler+SoftImpute
"""

def runTrain():
  print("Training Models...")
  best_models = {
      KNN(3,verbose=False):['X8','X9','X11'],
      KNN(4,verbose=False):['X1','X3','X7','X10','X12'],
      KNN(6,verbose=False):['X2'],
      KNN(8,verbose=False):['X13'],
      SoftImpute(verbose=False):['X4'],
      SoftImpute(verbose=False):['X5','X6']
  }

  """### Run on Train"""

  knndf = pd.DataFrame([],columns=['ptnum','time','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
  knndf_gt = pd.DataFrame([],columns=['ptnum','time','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
  for file in os.listdir("train_data/train_with_missing"):
      if file.endswith(".csv"):
          filepath = os.path.join("train_data/train_with_missing", file)
          df_pt = pd.read_csv(filepath)
          filepath = os.path.join("train_data/train_groundtruth", file)
          df_gt = pd.read_csv(filepath)
          ptnum = int(file.split(".")[0])
          df_oth = df_pt.iloc[:,1:]
          df_time = df_pt.iloc[:,0]
          df_pt = pd.concat([df_oth,df_time],axis=1)
          df_oth = df_gt.iloc[:,1:]
          df_time = df_gt.iloc[:,0]
          df_gt = pd.concat([df_oth,df_time],axis=1)
          df_pt["ptnum"] = ptnum
          df_gt["ptnum"] = ptnum
          mins,maxs = feature_engg(df_pt,returnmin=True)
          df_numeric = df_pt.select_dtypes(include=[np.float]).as_matrix()
          df_imputed = pd.DataFrame(index=df_pt.index,columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
          for i,model in enumerate(best_models):
            if i != 5:
              df_filled = pd.DataFrame(model.fit_transform(df_numeric), columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','timediff_1','timediff_5','timediff_median'])
            else:
              df_filled = pd.DataFrame(model.fit_transform(df_numeric[:,0:13]), columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
            for col in best_models[model]:
              df_imputed[col] = df_filled[col]
      
          df_imputed["ptnum"] = df_pt["ptnum"]
          df_imputed["time"] = df_pt["time"]
          
          for i in range(len(df_imputed.columns)):
            name = df_imputed.iloc[:,i].name
            if name in maxs:
              max_val = maxs[name]
              min_val = mins[name]
              df_imputed[name] = df_imputed[name].map(lambda x: x*(max_val-min_val) + min_val)
          
          knndf = knndf.append(df_imputed,ignore_index=True)
          knndf_gt = knndf_gt.append(df_gt,ignore_index=True)
          
          
  knndf = knndf.sort_values(['ptnum','time'],ascending=[1,1]).reset_index(drop=True)
  knndf_gt = knndf_gt.sort_values(['ptnum','time'],ascending=[1,1]).reset_index(drop=True)

  # Evaluate NRMS
  ### Print nrms per feature and average nrms
  imputed = knndf.copy()
  rmsList = []
  for i in range(1,14):
    print(i)
    imputed["X"+str(i)+"_groundtruth"] = knndf_gt["X"+str(i)]
    imputed["X"+str(i)+"_masked"] = df["masked_X"+str(i)]
    rms = nrms(imputed["X"+str(i)+"_masked"],imputed["X"+str(i)],imputed["X"+str(i)+"_groundtruth"],imputed["ptnum"])
    print(rms)
    rmsList.append(rms)
  print(rmsList)

"""# HYPERPARAMETER TUNING
### Run this code if you want to do hyperparameter tuning. Change the model parameter to test different models
"""
def tune():
  print("Tuning for k= 2 to 10")
  min_rms = float('inf')
  min_k = -1
  for k in range(2,11):
    model = KNN(k,verbose=False) #or different k or SoftImpute or BiScaler or SimpleFill
    #Read Data
    knndf = pd.DataFrame([],columns=['ptnum','time','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','timediff_1','timediff_5','timediff_median'])
    for file in os.listdir("train_data/train_with_missing"):
        if file.endswith(".csv"):
            filepath = os.path.join("train_data/train_with_missing", file)
            df_pt = pd.read_csv(filepath)
            ptnum = int(file.split(".")[0])
            df_oth = df_pt.iloc[:,1:]
            df_time = df_pt.iloc[:,0]
            df_pt = pd.concat([df_oth,df_time],axis=1)
            df_pt["ptnum"] = ptnum
            mins,maxs = feature_engg(df_pt,returnmin=True)
            df_numeric = df_pt.select_dtypes(include=[np.float]).as_matrix()
            df_filled = pd.DataFrame(model.fit_transform(df_numeric), columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','timediff_1','timediff_5','timediff_median'])
            df_filled["ptnum"] = df_pt["ptnum"]
            df_filled["time"] = df_pt["time"]

            for i in range(13):
              name = df_filled.iloc[:,i].name
              max_val = maxs[name]
              min_val = mins[name]
              df_filled[name] = df_filled[name].map(lambda x: x*(max_val-min_val) + min_val)

            knndf = knndf.append(df_filled,ignore_index=True)

    knndf = knndf.sort_values(['ptnum','time'],ascending=[1,1]).reset_index(drop=True)
    #Evaluate
    imputed = knndf.copy()
    rmsList = []
    for i in range(1,14):
      imputed["X"+str(i)+"_groundtruth"] = df2["X"+str(i)]
      imputed["X"+str(i)+"_masked"] = df["masked_X"+str(i)]
      rms = nrms(imputed["X"+str(i)+"_masked"],imputed["X"+str(i)],imputed["X"+str(i)+"_groundtruth"],imputed["ptnum"])
      print(k,i,rms)
      rmsList.append(rms)
    print(k,rmsList)
    avgrms = np.mean(rmsList)
    print(avgrms)
    if avgrms < min_rms:
      min_rms = avgrms
      min_k=k

"""# Run on Test"""
def runTest():
  print("Testing on Models")
  best_models = {
      KNN(3,verbose=False):['X8','X9','X11'],
      KNN(4,verbose=False):['X1','X3','X7','X10','X12'],
      KNN(6,verbose=False):['X2'],
      KNN(8,verbose=False):['X13'],
      SoftImpute(verbose=False):['X4'],
      SoftImpute(verbose=False):['X5','X6']
  }
  testImpute = pd.DataFrame([],columns=['ptnum','time','X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
  for file in os.listdir("test_data/test_with_missing"):
      if file.endswith(".csv"):
          filepath = os.path.join("test_data/test_with_missing", file)
          df_pt = pd.read_csv(filepath)
          ptnum = int(file.split(".")[0])
          df_oth = df_pt.iloc[:,1:]
          df_time = df_pt.iloc[:,0]
          df_pt = pd.concat([df_oth,df_time],axis=1)
          df_pt["ptnum"] = ptnum

          mins,maxs = feature_engg(df_pt,returnmin=True)
          df_numeric = df_pt.select_dtypes(include=[np.float]).as_matrix()
          df_imputed = pd.DataFrame(index=df_pt.index,columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
          for i,model in enumerate(best_models):
            if i != 5:
              df_filled = pd.DataFrame(model.fit_transform(df_numeric), columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','timediff_1','timediff_5','timediff_median'])
            else:
              df_filled = pd.DataFrame(model.fit_transform(df_numeric[:,0:13]), columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13'])
            for col in best_models[model]:
              df_imputed[col] = df_filled[col]
      
          df_imputed = pd.concat([pd.Series(df_pt["time"],index=df_pt.index,name="time"),df_imputed],axis=1)
          
          for i in range(len(df_imputed.columns)):
            name = df_imputed.iloc[:,i].name
            if name in maxs:
              max_val = maxs[name]
              min_val = mins[name]
              df_imputed[name] = df_imputed[name].map(lambda x: x*(max_val-min_val) + min_val)
          
          if not os.path.exists('test_imputed'):
            os.makedirs('test_imputed')
          df_imputed.to_csv('test_imputed/'+str(ptnum)+'.csv',index=False)
          
          testImpute = testImpute.append(df_imputed,ignore_index=True)
          
          
  testImpute = testImpute.sort_values(['ptnum','time'],ascending=[1,1]).reset_index(drop=True)
  print(testImpute.head())

if __name__ == '__main__':
  FLAGS = get_args()
  #load data
  df,df2 = load_data()
  if FLAGS.train:
    if not os.path.exists("train_data/train_with_missing") or not os.path.exists("train_data/train_groundtruth"):
      print("Make sure train data is present at train_data/train_with_missing and train_data/train_groundtruth")
    else:
      runTrain()
  if FLAGS.test:
    if not os.path.exists("test_data/test_with_missing"):
      print("Make sure test data is present at test_data/test_with_missing and test_data/test_groundtruth")
    else:
      runTest()
  if FLAGS.tune:
    if not os.path.exists("train_data/train_with_missing") or not os.path.exists("train_data/train_groundtruth"):
      print("Make sure train data is present at train_data/train_with_missing and train_data/train_groundtruth")
    else:
      tune()
